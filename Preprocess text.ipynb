{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "OOV_TOKEN = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.CsvDataset(os.path.join('data', 'reviews.csv'), \n",
    "                                          [tf.float32, tf.string], \n",
    "                                          header=True)\n",
    "dataset = dataset.map(lambda stars, text: (text, stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r'):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(filters, ' ' * len(filters)))\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea1c4c4ca4489389bea3802c19c31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_counts = OrderedDict()\n",
    "for text, _ in tqdm(dataset):\n",
    "    text = text.numpy().decode('utf-8')\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token in word_counts:\n",
    "            word_counts[token] += 1\n",
    "        else:\n",
    "            word_counts[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = list(word_counts.items())\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "vocabulary_list = [word for word, count in word_counts[:NUM_WORDS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172485\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counts))\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerEncoder:\n",
    "    def __init__(self, vocab_list, oov_token):\n",
    "        self.word_index = {word:(index+2) for index, word in enumerate(vocab_list)} # index 0 is reserved for padding token\n",
    "                                                                                    # index 1 is reserved for out of vocab token\n",
    "        self.word_index[oov_token] = 1\n",
    "        \n",
    "        self.reverse_word_index = {v: k for k, v in self.word_index.items()}\n",
    "        self.reverse_word_index[0] = '' # for decoding\n",
    "        \n",
    "        self.oov_token = oov_token\n",
    "    def encode(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        encoded_tokens = [self.word_index[token] if token in self.word_index \n",
    "                          else self.word_index[self.oov_token] for token in tokens]\n",
    "        return encoded_tokens\n",
    "    \n",
    "    def decode(self, encoded_tokens):\n",
    "        tokens = [self.reverse_word_index[idx] for idx in encoded_tokens]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerEncoder(vocab_list=vocabulary_list, oov_token=OOV_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[825, 577, 11, 19, 549, 45, 110, 1, 250, 8617, 283, 25, 2, 4925, 6, 552, 78, 7165, 11, 143, 8003, 4, 777, 752, 2, 8003, 72, 34, 25, 11, 2524, 2796, 290, 1003, 2071, 1, 30, 40, 1799]\n",
      "total bill for this horrible service over <UNK> these crooks actually had the nerve to charge us 69 for 3 pills i checked online the pills can be had for 19 cents each avoid hospital <UNK> at all costs\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode('Total\\r\\n bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.')\n",
    "print(encoded_tokens)\n",
    "print(tokenizer.decode(encoded_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(text, stars):\n",
    "    tf_text, tf_stars = tf.py_function(\n",
    "        func=lambda text, stars: (tokenizer.encode(text.numpy().decode('utf-8')), tf.cast(stars, tf.int64)), \n",
    "        inp=[text, stars],\n",
    "        Tout=(tf.int64, tf.int64)\n",
    "    )\n",
    "    return tf_text, tf_stars\n",
    "\n",
    "dataset = dataset.map(tf_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=3000427, shape=(39,), dtype=int64, numpy=\n",
       " array([ 825,  577,   11,   19,  549,   45,  110,    1,  250, 8617,  283,\n",
       "          25,    2, 4925,    6,  552,   78, 7165,   11,  143, 8003,    4,\n",
       "         777,  752,    2, 8003,   72,   34,   25,   11, 2524, 2796,  290,\n",
       "        1003, 2071,    1,   30,   40, 1799], dtype=int64)>,\n",
       " <tf.Tensor: id=3000428, shape=(), dtype=int64, numpy=1>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save `vocabulary_list` and `word_counts` to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vocabulary_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(word_counts, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    vocab_list = pickle.load(f)\n",
    "    wc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172485"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
